{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f56b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ciaran\n",
      "[nltk_data]     Mc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ciaran\n",
      "[nltk_data]     Mc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ciaran\n",
      "[nltk_data]     Mc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Ciaran\n",
      "[nltk_data]     Mc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import pkgs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from io import BytesIO\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e378c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold  Accuracy  Precision    Recall        F1       FPR           Metrics\n",
      "0     1  0.983871   0.986301  0.900000  0.941176  0.002092   [72, 1, 477, 8]\n",
      "1     2  0.983871   1.000000  0.861538  0.925620  0.000000   [56, 0, 493, 9]\n",
      "2     3  0.976661   1.000000  0.833333  0.909091  0.000000  [65, 0, 479, 13]\n",
      "3     4  0.991023   1.000000  0.933333  0.965517  0.000000   [70, 0, 482, 5]\n",
      "4     5  0.969479   0.985714  0.811765  0.890323  0.002119  [69, 1, 471, 16]\n",
      "5     6  0.983842   0.985915  0.897436  0.939597  0.002088   [70, 1, 478, 8]\n",
      "6     7  0.965889   1.000000  0.724638  0.840336  0.000000  [50, 0, 488, 19]\n",
      "7     8  0.987433   1.000000  0.904110  0.949640  0.000000   [66, 0, 484, 7]\n",
      "8     9  0.973070   0.985507  0.829268  0.900662  0.002105  [68, 1, 474, 14]\n",
      "9    10  0.980251   0.981132  0.838710  0.904348  0.002020  [52, 1, 494, 10]\n",
      "7.938269376754761\n"
     ]
    }
   ],
   "source": [
    "#instantiate start time\n",
    "start_time = time.time()\n",
    "\n",
    "#load data from github\n",
    "url = 'https://raw.githubusercontent.com/cmcswiggan/CIND820/main/SMS_Spam_Dataset'\n",
    "data = requests.get(url).content\n",
    "smsData = pd.read_csv(BytesIO(data), sep = '\\t', header = None, names = ['Category', 'SMS Message'])\n",
    "\n",
    "#assign stopwords and punctuation to variables\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "#pre_Process function defined - list required for lemmatization step\n",
    "\n",
    "def pre_Process(sms):\n",
    "    \n",
    "    #remove punctuation, toeknize and remove stopwords\n",
    "    process = \"\".join([char.lower() for char in sms if char not in punctuation])\n",
    "    tokenize = nltk.tokenize.word_tokenize(process)\n",
    "    remove_stopwords = [word for word in tokenize if word not in stopWords]\n",
    "    return remove_stopwords\n",
    "\n",
    "#add processed column\n",
    "smsData['Processed'] = smsData['SMS Message'].apply(lambda x: pre_Process(x))\n",
    "\n",
    "#define function for lemmatizing words\n",
    "\n",
    "def lemm (sms):\n",
    "    \n",
    "    lem = [lemmatizer.lemmatize(word) for word in sms]\n",
    "    return lem\n",
    "\n",
    "#replace processed column with lemmatized column\n",
    "smsData['Processed'] = smsData['Processed'].apply(lambda x: lemm(x))\n",
    "\n",
    "#add cell with processed data as a string after processing\n",
    "smsData['Processedstr'] = [' '.join(map(str, l)) for l in smsData['Processed']]\n",
    "\n",
    "#assign 0 for ham and 1 for spam\n",
    "for i in range(len(smsData['Category'])):\n",
    "    if smsData.iloc[i, 0] == 'ham':\n",
    "        smsData.iloc[i, 0] = 1\n",
    "    else:\n",
    "        smsData.iloc[i, 0] = 0\n",
    "\n",
    "#remove unwanted columns\n",
    "smsData = smsData[['Category', 'Processedstr']]\n",
    "\n",
    "#split data into 10 folds to run model on each fold\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 5)\n",
    "\n",
    "#counter\n",
    "i = 0\n",
    "\n",
    "#initiate empty lists and data frame\n",
    "fold = []\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "fprt = []\n",
    "met = []\n",
    "res = pd.DataFrame()\n",
    "\n",
    "#split data/loop over each fold and run model \n",
    "for train_index, test_index in kf.split(smsData.Processedstr, smsData.Category):\n",
    "    sms_train, sms_test, label_train, label_test = smsData.Processedstr[train_index], smsData.Processedstr[test_index], smsData.Category[train_index], smsData.Category[test_index]\n",
    "    \n",
    "    #labels as integers\n",
    "    label_train = label_train.astype('int')\n",
    "    label_test = label_test.astype('int')\n",
    "    \n",
    "    #fit and transform training and test data\n",
    "    vectorizer = TfidfVectorizer(min_df=10)\n",
    "\n",
    "    train_transformed = vectorizer.fit_transform(sms_train)\n",
    "\n",
    "    test_transformed = vectorizer.transform(sms_test)\n",
    "        \n",
    "    #SVM classifier creation\n",
    "    clf = SVC()\n",
    "    clf.fit(train_transformed, label_train)\n",
    "\n",
    "    #assign predictor\n",
    "    train_predict = clf.predict(test_transformed)\n",
    "\n",
    "    #run confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(label_test, train_predict, labels = [1, 0]).ravel()\n",
    "    \n",
    "    #calculate accuracy, precision, recall, F1 score, False Positive Rate\n",
    "    a = (tp + tn)/(tp + fp + fn + tn)\n",
    "    p = tp/(tp + fp)\n",
    "    r = tp/(tp + fn)\n",
    "    f = (2 * (p * r))/(p + r)\n",
    "    fpr = fp / (tn + fp)\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    #append results to each empty list\n",
    "    fold.append(i)\n",
    "    acc.append(a)\n",
    "    prec.append(p)\n",
    "    rec.append(r)\n",
    "    f1.append(f)\n",
    "    fprt.append(fpr)\n",
    "    met.append([tp, fp, tn, fn])\n",
    "\n",
    "#fill dataframe with results from the model/cross-validation \n",
    "res['Fold'] = fold    \n",
    "res['Accuracy'] = acc\n",
    "res['Precision'] = prec\n",
    "res['Recall'] = rec\n",
    "res['F1'] = f1\n",
    "res['FPR'] = fprt\n",
    "res['Metrics'] = met\n",
    "\n",
    "#verify modified model has the same results\n",
    "print(res)\n",
    "\n",
    "#print elapsed time\n",
    "print((time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
